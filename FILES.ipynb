{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f9ffb70-5f0c-4b75-a2c7-19aeaadd2c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the list to store users from Delhi\n",
    "users_in_zurich = []\n",
    "\n",
    "# Read the CSV file with UTF-8 encoding\n",
    "with open(r\"C:\\Users\\Lenovo\\Desktop\\tds\\users.csv\", 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        location = row['location'].strip().lower()\n",
    "        # Check if the user is from Delhi\n",
    "        if 'delhi' in location:\n",
    "            users_in_delhi.append({\n",
    "                'login': row['login'],\n",
    "                'followers': int(row['followers'])\n",
    "            })\n",
    "\n",
    "# Sort users based on followers in descending order\n",
    "top_users = sorted(users_in_zurich, key=lambda x: x['followers'], reverse=True)\n",
    "\n",
    "# Extract the top 5 user logins\n",
    "top_5_logins = [user['login'] for user in top_users[:5]]\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_5_logins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "457726cb-bad3-4b63-bac3-8c159bfd0954",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (317373613.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[35], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"C:\\Users\\Lenovo\\Desktop\\tds\\repositories.csv\"\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\"C:\\Users\\Lenovo\\Desktop\\tds\\repositories.csv\"\n",
    "\"C:\\Users\\Lenovo\\Desktop\\tds\\users.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f5919226-49c0-4776-9ca4-fcbf8cf989da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDouble,TheOfficialFloW,Seldaek,riscv,JonnyBurger\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\TDS-Project1-main\\users.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter users from Zurich\n",
    "zurich_users = df[df['location'].str.contains('Zurich', case=False, na=False)]\n",
    "\n",
    "# Sort by followers in descending order\n",
    "zurich_users_sorted = zurich_users.sort_values(by='followers', ascending=False)\n",
    "\n",
    "# Get the top 5 users\n",
    "top_5_users = zurich_users_sorted.head(5)\n",
    "\n",
    "# Extract the login names\n",
    "top_5_logins = top_5_users['login'].tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_5_logins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "109f6e01-64a9-48be-af46-d55d50f0f3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDouble,TheOfficialFloW,Seldaek,riscv,JonnyBurger\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\tds\\users.csv\", encoding= \"utf-8\")\n",
    "\n",
    "# Filter users from Zurich\n",
    "zurich_users = df[df['location'].str.contains('Zurich', case=False, na=False)]\n",
    "\n",
    "# Sort by followers in descending order\n",
    "zurich_users_sorted = zurich_users.sort_values(by='followers', ascending=False)\n",
    "\n",
    "# Get the top 5 users\n",
    "top_5_users = zurich_users_sorted.head(5)\n",
    "\n",
    "# Extract the login names\n",
    "top_5_logins = top_5_users['login'].tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_5_logins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "568b0997-9501-4295-a596-40c592fedb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lejoe,uwolfer,matthiask,oscardelben,panterch\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "# Define the list to store users from Zurich\n",
    "users_in_zurich = []\n",
    "\n",
    "# Read the CSV file with UTF-8 encoding\n",
    "with open(r\"C:\\Users\\Lenovo\\Desktop\\tds\\users.csv\", 'r', encoding='utf-8') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    for row in reader:\n",
    "        location = row['location'].strip().lower()\n",
    "        # Check if the user is from Zurich\n",
    "        if 'zurich' in location:\n",
    "            users_in_zurich.append({\n",
    "                'login': row['login'],\n",
    "                'created_at': row['created_at']\n",
    "            })\n",
    "\n",
    "# Sort users by the created_at date in ascending order\n",
    "sorted_users = sorted(users_in_zurich, key=lambda x: x['created_at'])\n",
    "\n",
    "# Get the top 5 earliest registered users\n",
    "top_5_users = sorted_users[:5]\n",
    "top_5_user_logins = [user['login'] for user in top_5_users]\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_5_user_logins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f06e328b-2c9a-4cec-b010-a36dc3c339d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No license,MIT License,Apache License 2.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\TDS-Project1-main\\repositories.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter out rows with missing license_name\n",
    "filtered_df = df.dropna(subset=['license_name'])\n",
    "\n",
    "# Count the occurrences of each license\n",
    "license_counts = filtered_df['license_name'].value_counts()\n",
    "\n",
    "# Get the top 3 licenses\n",
    "top_3_licenses = license_counts.head(3).index.tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_3_licenses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55670387-9002-43ce-8cc5-80a9f79e3136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The majority of Zurich-based developers work at: GOOGLE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter users from Zurich\n",
    "zurich_users = df[df['location'].str.contains('Zurich', case=False, na=False)]\n",
    "\n",
    "# Count the occurrences of each company\n",
    "company_counts = zurich_users['company'].value_counts()\n",
    "\n",
    "# Get the most common company\n",
    "most_common_company = company_counts.idxmax()\n",
    "\n",
    "print(\"The majority of Zurich-based developers work at:\", most_common_company)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af70fb5-9267-4330-8cee-5d0b1b855ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most popular programming language among Zurich-based users is: Python\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter out rows with missing language\n",
    "filtered_df = df.dropna(subset=['language'])\n",
    "\n",
    "# Count the occurrences of each programming language\n",
    "language_counts = filtered_df['language'].value_counts()\n",
    "\n",
    "# Get the most popular language\n",
    "most_popular_language = language_counts.idxmax()\n",
    "\n",
    "print(\"The most popular programming language among Zurich-based users is:\", most_popular_language)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291374f9-d0c3-4a3a-abe8-4dab831217c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The language with the highest average stars per repository is: BitBake with an average of 363.00 stars per repository.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter out rows with missing language and stars\n",
    "filtered_df = df.dropna(subset=['language', 'stargazers_count'])\n",
    "\n",
    "# Group by language and calculate the average stars\n",
    "average_stars_per_language = filtered_df.groupby('language')['stargazers_count'].mean()\n",
    "\n",
    "# Get the language with the highest average stars\n",
    "most_popular_language = average_stars_per_language.idxmax()\n",
    "highest_average_stars = average_stars_per_language.max()\n",
    "\n",
    "print(f\"The language with the highest average stars per repository is: {most_popular_language} with an average of {highest_average_stars:.2f} stars per repository.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13f16521-07f6-4393-ad3d-d7cff4af354f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riscv,bpasero,Seldaek,egamma,ethz-asl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Calculate leader_strength\n",
    "df['leader_strength'] = df['followers'] / (1 + df['following'])\n",
    "\n",
    "# Sort users by leader_strength in descending order\n",
    "sorted_users = df.sort_values(by='leader_strength', ascending=False)\n",
    "\n",
    "# Get the top 5 users\n",
    "top_5_users = sorted_users.head(5)\n",
    "\n",
    "# Extract the login names\n",
    "top_5_logins = top_5_users['login'].tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(','.join(top_5_logins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b9630f-72af-42c2-95d9-a0a3bb61f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between the number of followers and the number of public repositories among users in Zurich is: 0.06615227419682651\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter users from Zurich\n",
    "zurich_users = df[df['location'].str.contains('Zurich', case=False, na=False)]\n",
    "\n",
    "# Calculate the correlation\n",
    "correlation = zurich_users['followers'].corr(zurich_users['public_repos'])\n",
    "\n",
    "print(\"The correlation between the number of followers and the number of public repositories among users in Zurich is:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "38df41f2-6364-4b03-b15c-93596b70655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated increase in followers per additional public repository: 1.46\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\TDS-Project1-main\\users.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter users from Zurich\n",
    "zurich_users = df[df['location'].str.contains('Zurich', case=False, na=False)]\n",
    "\n",
    "# Prepare the data\n",
    "X = zurich_users[['public_repos']].values.reshape(-1, 1)\n",
    "y = zurich_users['followers'].values\n",
    "\n",
    "# Create and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the slope of the regression line (coefficient)\n",
    "slope = model.coef_[0]\n",
    "\n",
    "print(f\"Estimated increase in followers per additional public repository: {slope:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b009c1e-8fee-4742-9fe0-3c85f7d9add3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between having projects enabled and having wiki enabled is: 0.351\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\TDS-Project1-main\\repositories.csv\", encoding='utf-8')\n",
    "\n",
    "# Convert 'has_projects' and 'has_wiki' to numeric (0 and 1)\n",
    "df['has_projects'] = df['has_projects'].astype(bool)\n",
    "df['has_wiki'] = df['has_wiki'].astype(bool)\n",
    "\n",
    "# Calculate the correlation between 'has_projects' and 'has_wiki'\n",
    "correlation = df['has_projects'].corr(df['has_wiki'])\n",
    "\n",
    "print(f\"The correlation between having projects enabled and having wiki enabled is: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a0ca4861-9abc-4d7d-bd98-0eed0b47d908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Statistic: 2503.12110856974\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = r\"D:\\Users\\ujjwa\\Videos\\Valorant\\repositories.csv\"  # Replace with the correct path\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert 'has_projects' and 'has_wiki' to boolean if necessary\n",
    "df['has_projects'] = df['has_projects'].astype(bool)\n",
    "df['has_wiki'] = df['has_wiki'].astype(bool)\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['has_projects'], df['has_wiki'])\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8df730d3-3af8-4bad-8f49-7fb02144f7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation between having projects enabled and having wiki enabled is: 0.3502779665537579\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Convert 'has_projects' and 'has_wiki' to numeric (0 and 1)\n",
    "df['has_projects'] = df['has_projects'].astype(int)\n",
    "df['has_wiki'] = df['has_wiki'].astype(int)\n",
    "\n",
    "# Calculate the correlation between 'has_projects' and 'has_wiki'\n",
    "correlation = df['has_projects'].corr(df['has_wiki'])\n",
    "\n",
    "print(\"The correlation between having projects enabled and having wiki enabled is:\", correlation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c589c1a-07b6-4463-a7a2-64a6639ed3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chi-Square Statistic: 1425.9229527736725\n",
      "P-value: 0.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\"  # Replace with the correct path\n",
    "\n",
    "# Load the CSV into a DataFrame\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert 'has_projects' and 'has_wiki' to boolean if necessary\n",
    "df['has_projects'] = df['has_projects'].astype(bool)\n",
    "df['has_wiki'] = df['has_wiki'].astype(bool)\n",
    "\n",
    "# Create a contingency table\n",
    "contingency_table = pd.crosstab(df['has_projects'], df['has_wiki'])\n",
    "\n",
    "# Perform Chi-Square test\n",
    "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"Chi-Square Statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa9d99a4-7ecf-462a-9d18-6ec00a800bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of repositories with both projects and wikis enabled: 84.65%\n",
      "Correlation between has_projects and has_wiki: 0.350\n",
      "has_wiki      False  True \n",
      "has_projects              \n",
      "False           298     46\n",
      "True           1447   9878\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the repositories.csv file\n",
    "repositories_df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\")\n",
    "\n",
    "# Display the first few rows of the data to verify\n",
    "repositories_df.head()\n",
    "\n",
    "# Calculate the number of repos with both projects and wikis enabled\n",
    "both_enabled = repositories_df[(repositories_df['has_projects'] == True) & (repositories_df['has_wiki'] == True)].shape[0]\n",
    "\n",
    "# Calculate the total number of repositories\n",
    "total_repos = repositories_df.shape[0]\n",
    "\n",
    "# Calculate the percentage\n",
    "percentage_both_enabled = (both_enabled / total_repos) * 100\n",
    "\n",
    "print(f\"Percentage of repositories with both projects and wikis enabled: {percentage_both_enabled:.2f}%\")\n",
    "\n",
    "# Calculate the correlation between has_projects and has_wiki\n",
    "correlation = repositories_df['has_projects'].corr(repositories_df['has_wiki'])\n",
    "\n",
    "print(f\"Correlation between has_projects and has_wiki: {correlation:.3f}\")\n",
    "\n",
    "\n",
    "# Create a cross-tabulation of has_projects and has_wiki\n",
    "cross_tab = pd.crosstab(repositories_df['has_projects'], repositories_df['has_wiki'])\n",
    "\n",
    "print(cross_tab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfc889e7-98d9-47e3-a92f-c476c181f26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of people followed by hireable users: 75.72\n",
      "Average number of people followed by non-hireable users: 907.35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Calculate the average number of people followed by hireable and non-hireable users\n",
    "avg_following_hireable = df[df['hireable'] == True]['following'].mean()\n",
    "avg_following_not_hireable = df[df['hireable'] == False]['following'].mean()\n",
    "\n",
    "print(f\"Average number of people followed by hireable users: {avg_following_hireable:.2f}\")\n",
    "print(f\"Average number of people followed by non-hireable users: {avg_following_not_hireable:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e8b238-5f1b-4636-a38e-04549841eeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference: -831.631\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Calculate the average number of people followed by hireable users\n",
    "avg_following_hireable = df[df['hireable'] == True]['following'].mean()\n",
    "\n",
    "# Calculate the average number of people followed by non-hireable users\n",
    "avg_following_not_hireable = df[df['hireable'] == False]['following'].mean()\n",
    "\n",
    "# Calculate the difference\n",
    "difference = avg_following_hireable - avg_following_not_hireable\n",
    "\n",
    "print(f\"Difference: {difference:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47257427-c67a-44e2-b87a-05948ada4b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between has_projects and has_wiki: 0.350\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the repositories.csv file\n",
    "repositories_df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\")\n",
    "\n",
    "# Calculate the correlation between has_projects and has_wiki\n",
    "correlation = repositories_df['has_projects'].corr(repositories_df['has_wiki'])\n",
    "\n",
    "# Print the correlation rounded to 3 decimal places\n",
    "print(f\"Correlation between has_projects and has_wiki: {correlation:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca0d3322-12be-4be2-811f-977f3b152107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The second most popular programming language among users who joined after 2020 is: JavaScript\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the users and repositories CSV files into DataFrames\n",
    "users_df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "repos_df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter users who joined after 2020\n",
    "users_df['created_at'] = pd.to_datetime(users_df['created_at'])\n",
    "recent_users = users_df[users_df['created_at'] > '2020-01-01']\n",
    "\n",
    "# Get the logins of recent users\n",
    "recent_user_logins = recent_users['login']\n",
    "\n",
    "# Filter repositories by recent users\n",
    "recent_user_repos = repos_df[repos_df['login'].isin(recent_user_logins)]\n",
    "\n",
    "# Count the occurrences of each programming language\n",
    "language_counts = recent_user_repos['language'].value_counts()\n",
    "\n",
    "# Get the second most popular language\n",
    "second_most_popular_language = language_counts.index[1]\n",
    "\n",
    "print(f\"The second most popular programming language among users who joined after 2020 is: {second_most_popular_language}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0baf5e29-9e09-46a0-913a-86eaffb5e423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression slope of followers on bio word count: 5.410\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Filter out users without bios\n",
    "df = df[df['bio'].notna()]\n",
    "\n",
    "# Calculate the length of each bio in Unicode characters\n",
    "df['bio_length'] = df['bio'].apply(len)\n",
    "\n",
    "# Prepare the data for regression\n",
    "X = df['bio_length'].values.reshape(-1, 1)\n",
    "y = df['followers'].values\n",
    "\n",
    "# Perform the regression\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get the slope of the regression line (coefficient)\n",
    "slope = model.coef_[0]\n",
    "\n",
    "print(f\"Regression slope of followers on bio word count: {slope:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7508438-6ad6-406e-85d4-7eaa23ba2c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 users by repositories created on weekends: filiphr,stefanhaustein,iwangu,shuhei,yati-sagade\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\pro1\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Convert the 'created_at' column to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Filter out repositories created on weekends (5 = Saturday, 6 = Sunday)\n",
    "weekend_repos = df[df['day_of_week'].isin([5, 6])]\n",
    "\n",
    "# Count the number of repositories created by each user\n",
    "repo_counts = weekend_repos['login'].value_counts()\n",
    "\n",
    "# Get the top 5 users\n",
    "top_5_users = repo_counts.head(5).index.tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(\"Top 5 users by repositories created on weekends:\", \",\".join(top_5_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf72802-1427-4925-8182-ba71214729a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "620a4c80-8854-4162-8e28-dc7ab8c4f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 users by repositories created on weekends: filiphr,stefanhaustein,iwangu,shuhei,yati-sagade\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\pro1\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Convert the 'created_at' column to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Filter out repositories created on weekends (5 = Saturday, 6 = Sunday)\n",
    "weekend_repos = df[df['day_of_week'].isin([5, 6])]\n",
    "\n",
    "# Count the number of repositories created by each user\n",
    "repo_counts = weekend_repos['login'].value_counts()\n",
    "\n",
    "# Get the top 5 users, sorted by the number of repositories created\n",
    "top_5_users = repo_counts.head(5).index.tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(\"Top 5 users by repositories created on weekends:\", \",\".join(top_5_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcff6499-3b1c-4e61-978b-a028782e36ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average following difference (hireable - non-hireable): nan\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the users.csv file\n",
    "users_df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\")\n",
    "\n",
    "# Calculate the average number of people followed by hireable users\n",
    "avg_following_hireable = users_df[users_df['hireable'] == 'true']['following'].mean()\n",
    "\n",
    "# Calculate the average number of people followed by non-hireable users (hireable = False or NaN)\n",
    "avg_following_non_hireable = users_df[users_df['hireable'] != 'true']['following'].mean()\n",
    "\n",
    "# Calculate the difference\n",
    "difference = avg_following_hireable - avg_following_non_hireable\n",
    "\n",
    "# Print the result rounded to 3 decimal places\n",
    "print(f\"Average following difference (hireable - non-hireable): {difference:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2cd3f5-598d-4b95-9f41-05f730d9046c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4011bfac-1366-4b2a-86fa-9414af992088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 users by repositories created on weekends: filiphr,stefanhaustein,iwangu,shuhei,WhiteFireFox\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\repositories1.csv\", encoding='utf-8')\n",
    "\n",
    "# Convert the 'created_at' column to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Filter out repositories created on weekends (5 = Saturday, 6 = Sunday)\n",
    "weekend_repos = df[df['day_of_week'].isin([5, 6])]\n",
    "\n",
    "# Count the number of repositories created by each user\n",
    "repo_counts = weekend_repos['login'].value_counts()\n",
    "\n",
    "# Get the top 5 users, sorted by the number of repositories created\n",
    "top_5_users = repo_counts.head(5).index.tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(\"Top 5 users by repositories created on weekends:\", \",\".join(top_5_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cc5fc3c-7468-47da-9193-5bbcd58fffb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of hireable users who share their email addresses: 53.398058252427184%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your CSV file with user data into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Check if the 'email' column is not null and the 'hireable' column is True\n",
    "hireable_emails = df[df['email'].notnull() & df['hireable']]\n",
    "\n",
    "# Calculate the percentage of hireable users who share their email addresses\n",
    "percentage = (len(hireable_emails) / len(df[df['hireable']])) * 100\n",
    "\n",
    "# Print the result\n",
    "print(f\"Percentage of hireable users who share their email addresses: {percentage:}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e5cad78-e880-4325-9e53-b3245c15e528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 users by repositories created on weekends: dheeraj-thedev, coding-blocks-archives, Ayush7614, imvickykumar999, dineshkummarc\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('repositories.csv', encoding='utf-8')\n",
    "\n",
    "# Convert the 'created_at' column to datetime\n",
    "df['created_at'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Extract the day of the week (0 = Monday, 6 = Sunday)\n",
    "df['day_of_week'] = df['created_at'].dt.dayofweek\n",
    "\n",
    "# Filter out repositories created on weekends (5 = Saturday, 6 = Sunday)\n",
    "weekend_repos = df[df['day_of_week'].isin([5, 6])]\n",
    "\n",
    "# Count the number of repositories created by each user\n",
    "repo_counts = weekend_repos['login'].value_counts()\n",
    "\n",
    "# Get the top 5 users, sorted by the number of repositories created\n",
    "top_5_users = repo_counts.head(5).index.tolist()\n",
    "\n",
    "# Print the result as a comma-separated list\n",
    "print(\"Top 5 users by repositories created on weekends:\", \", \".join(top_5_users))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f56041d6-01cd-4fbe-a79b-a0b029d0a16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common surname(s): Li, Wang\n",
      "Number of users with the most common surname: 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Load the CSV file with user data into a DataFrame\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\pro1\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Drop missing names\n",
    "df = df.dropna(subset=['name'])\n",
    "\n",
    "# Split names and get the last word (assumed to be the surname)\n",
    "df['surname'] = df['name'].apply(lambda x: x.strip().split()[-1])\n",
    "\n",
    "# Count the occurrences of each surname\n",
    "surname_counts = Counter(df['surname'])\n",
    "\n",
    "# Find the highest count\n",
    "max_count = max(surname_counts.values())\n",
    "\n",
    "# Find all surnames with the highest count\n",
    "most_common_surnames = sorted([surname for surname, count in surname_counts.items() if count == max_count])\n",
    "\n",
    "# Get the number of users with the most common surname(s)\n",
    "num_users = max_count\n",
    "\n",
    "# Print the results\n",
    "print(f\"Most common surname(s): {', '.join(most_common_surnames)}\")\n",
    "print(f\"Number of users with the most common surname: {num_users}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0900c34-2df7-4422-9617-8e5a87bbad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b877f-972d-4194-9c7a-70e77f0b6045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3a90b2c4-72db-4605-8747-06086de7dbd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot mask with non-boolean array containing NA / NaN values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m})\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the average following for hireable users\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m average_following_hireable \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollowing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Calculate the average following for non-hireable users\u001b[39;00m\n\u001b[0;32m     13\u001b[0m average_following_non_hireable \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m~\u001b[39mdf[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollowing\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:4092\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwhere(key)\n\u001b[0;32m   4091\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[1;32m-> 4092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m   4093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_bool_array(key)\n\u001b[0;32m   4095\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   4096\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\common.py:136\u001b[0m, in \u001b[0;36mis_bool_indexer\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m    132\u001b[0m     na_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot mask with non-boolean array containing NA / NaN values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mis_bool_array(key_array, skipna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;66;03m# Don't raise on e.g. [\"A\", \"B\", np.nan], see\u001b[39;00m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;66;03m#  test_loc_getitem_list_of_labels_categoricalindex_with_na\u001b[39;00m\n\u001b[1;32m--> 136\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(na_msg)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot mask with non-boolean array containing NA / NaN values"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the user data from the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Ensure the 'hireable' column is boolean\n",
    "df['hireable'] = df['hireable'].map({'true': True, 'false': False})\n",
    "\n",
    "# Calculate the average following for hireable users\n",
    "average_following_hireable = df[df['hireable']]['following'].mean()\n",
    "\n",
    "# Calculate the average following for non-hireable users\n",
    "average_following_non_hireable = df[~df['hireable']]['following'].mean()\n",
    "\n",
    "# Calculate the difference\n",
    "difference = average_following_hireable - average_following_non_hireable\n",
    "\n",
    "# Print the result to 3 decimal places\n",
    "print(f\"Difference in average following: {difference:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df2a8105-2873-42ef-8ca1-cf4a98b9b16b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\users1.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the user data from the CSV file\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mLenovo\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124musers1.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Replace NaN values in 'hireable' column with 'false'\u001b[39;00m\n\u001b[0;32m      7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhireable\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Lenovo\\\\Desktop\\\\users1.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the user data from the CSV file\n",
    "df = pd.read_csv(r\"C:\\Users\\Lenovo\\Desktop\\users1.csv\", encoding='utf-8')\n",
    "\n",
    "# Replace NaN values in 'hireable' column with 'false'\n",
    "df['hireable'] = df['hireable'].fillna('false')\n",
    "\n",
    "# Ensure the 'hireable' column is boolean\n",
    "df['hireable'] = df['hireable'].map({'true': True, 'false': False})\n",
    "\n",
    "# Calculate the average following for hireable users\n",
    "average_following_hireable = df[df['hireable']]['following'].mean()\n",
    "\n",
    "# Calculate the average following for non-hireable users\n",
    "average_following_non_hireable = df[~df['hireable']]['following'].mean()\n",
    "\n",
    "# Calculate the difference\n",
    "difference = average_following_hireable - average_following_non_hireable\n",
    "\n",
    "# Print the result to 3 decimal places\n",
    "print(f\"Difference in average following: {difference:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4bc72f1-ef57-44be-8016-c7f729a0fed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of users1.csv: (472, 11)\n",
      "Shape of repositories1.csv: (11669, 9)\n"
     ]
    }
   ],
   "source": [
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3e7589-c322-4f79-80bf-4e38e6335e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class GitHubScraper:\n",
    "    def _init_(self, token: str):\n",
    "        \"\"\"\n",
    "        Initialize the GitHub scraper with your API token.\n",
    "        \n",
    "        Args:\n",
    "            token (str): GitHub Personal Access Token\n",
    "        \"\"\"\n",
    "        self.headers = {\n",
    "            'Authorization': f'token {token}',\n",
    "            'Accept': 'application/vnd.github.v3+json'\n",
    "        }\n",
    "        self.base_url = 'https://api.github.com'\n",
    "        \n",
    "        # Setup logging\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "        )\n",
    "        self.logger = logging.getLogger(_name_)\n",
    "\n",
    "    def _make_request(self, url: str, params: dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Make a request to the GitHub API with rate limit handling.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            response = requests.get(url, headers=self.headers, params=params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json()\n",
    "            elif response.status_code == 403:\n",
    "                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "                sleep_time = max(reset_time - time.time(), 0) + 1\n",
    "                self.logger.warning(f\"Rate limit hit. Sleeping for {sleep_time} seconds\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                self.logger.error(f\"Error {response.status_code}: {response.text}\")\n",
    "                response.raise_for_status()\n",
    "\n",
    "    def clean_company_name(self, company: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up company names according to specifications.\n",
    "        \"\"\"\n",
    "        if not company:\n",
    "            return \"\"\n",
    "        \n",
    "        # Strip whitespace and @ symbol\n",
    "        cleaned = company.strip().lstrip('@')\n",
    "        \n",
    "        # Convert to uppercase\n",
    "        return cleaned.upper()\n",
    "\n",
    "    def search_users(self, location: str, min_followers: int) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for GitHub users in a specific location with minimum followers.\n",
    "        \"\"\"\n",
    "        users = []\n",
    "        page = 1\n",
    "        \n",
    "        while True:\n",
    "            self.logger.info(f\"Fetching users page {page}\")\n",
    "            \n",
    "            query = f\"location:{location} followers:>={min_followers}\"\n",
    "            params = {\n",
    "                'q': query,\n",
    "                'per_page': 100,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "            url = f\"{self.base_url}/search/users\"\n",
    "            response = self._make_request(url, params)\n",
    "            \n",
    "            if not response['items']:\n",
    "                break\n",
    "                \n",
    "            for user in response['items']:\n",
    "                user_data = self._make_request(user['url'])\n",
    "                \n",
    "                # Extract only the required fields with exact matching names\n",
    "                cleaned_data = {\n",
    "                    'login': user_data['login'],\n",
    "                    'name': user_data['name'] if user_data['name'] else \"\",\n",
    "                    'company': self.clean_company_name(user_data.get('company')),\n",
    "                    'location': user_data['location'] if user_data['location'] else \"\",\n",
    "                    'email': user_data['email'] if user_data['email'] else \"\",\n",
    "                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,\n",
    "                    'bio': user_data['bio'] if user_data['bio'] else \"\",\n",
    "                    'public_repos': user_data['public_repos'],\n",
    "                    'followers': user_data['followers'],\n",
    "                    'following': user_data['following'],\n",
    "                    'created_at': user_data['created_at']\n",
    "                }\n",
    "                \n",
    "                users.append(cleaned_data)\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        return users\n",
    "\n",
    "    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get repositories for a specific user.\n",
    "        \"\"\"\n",
    "        repos = []\n",
    "        page = 1\n",
    "        \n",
    "        while len(repos) < max_repos:\n",
    "            self.logger.info(f\"Fetching repositories for {username}, page {page}\")\n",
    "            \n",
    "            params = {\n",
    "                'sort': 'pushed',\n",
    "                'direction': 'desc',\n",
    "                'per_page': 100,\n",
    "                'page': page\n",
    "            }\n",
    "            \n",
    "            url = f\"{self.base_url}/users/{username}/repos\"\n",
    "            response = self._make_request(url, params)\n",
    "            \n",
    "            if not response:\n",
    "                break\n",
    "                \n",
    "            for repo in response:\n",
    "                # Extract only the required fields with exact matching names\n",
    "                repo_data = {\n",
    "                    'login': username,  # Adding owner's login as required\n",
    "                    'full_name': repo['full_name'],\n",
    "                    'created_at': repo['created_at'],\n",
    "                    'stargazers_count': repo['stargazers_count'],\n",
    "                    'watchers_count': repo['watchers_count'],\n",
    "                    'language': repo['language'] if repo['language'] else \"\",\n",
    "                    'has_projects': repo['has_projects'],\n",
    "                    'has_wiki': repo['has_wiki'],\n",
    "                    'license_name': repo['license']['key'] if repo.get('license') else \"\"\n",
    "                }\n",
    "                \n",
    "                repos.append(repo_data)\n",
    "                \n",
    "            if len(response) < 100:\n",
    "                break\n",
    "                \n",
    "            page += 1\n",
    "            \n",
    "        return repos[:max_repos]\n",
    "\n",
    "def main():\n",
    "    # Get GitHub token\n",
    "    token = input(\"Enter your GitHub token: \").strip()\n",
    "    if not token:\n",
    "        print(\"Token is required. Exiting...\")\n",
    "        return\n",
    "\n",
    "    # Initialize scraper\n",
    "    scraper = GitHubScraper(token)\n",
    "    \n",
    "    # Search for users in Delhi with >100 followers\n",
    "    users = scraper.search_users(location='Zurich', min_followers=50)\n",
    "    \n",
    "    # Save users to CSV\n",
    "    users_df = pd.DataFrame(users)\n",
    "    users_df.to_csv('users.csv', index=False)\n",
    "    \n",
    "    # Get repositories for each user\n",
    "    all_repos = []\n",
    "    for user in users:\n",
    "        repos = scraper.get_user_repositories(user['login'])\n",
    "        all_repos.extend(repos)\n",
    "    \n",
    "    # Save repositories to CSV\n",
    "    repos_df = pd.DataFrame(all_repos)\n",
    "    repos_df.to_csv('repositories.csv', index=False)\n",
    "    \n",
    "    print(f\"Scraped {len(users)} users and {len(all_repos)} repositories\")\n",
    "    \n",
    "  \n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
